Plot of the ablation available in batch_sizes_vs_counts.png in the main directory. All the other familiar logs, plots, and models available in their respective directories.

Inference instructions: As before, run `python inference.py model.pt X` where model.pt is the path to a model checkpoint and X is the maximum number of desired tokens. The data will be generated from scratch (0 context).

For our ablation experiment, we chose to test the effect of batch_size. The ablation plot indicates the number of optimization steps for test loss to be near 0 after train loss is near 0 (near 0 means < 1e-3). We tested batch sizes ranging from 32-1024. We expected that the higher batch size would cause the test loss to decay to close to zero more quickly than smaller batches due to simply training on more data. However we saw the opposite. Unsurprisingly, sometimes the model does not see enough samples for even train loss to decay enough at very small batch sizes (that is what the nonsensical -100 value indicates). However when train loss does converge, a lower batch_size causes test loss to converge more quickly relatively speaking. This might be because of the fact that test loss stays more closely coupled to train loss throughout the training time due to the model not-overtraining/memorizing the training data. It could also be simply due to random chance -- we sometimes observed that the random starting seed could have major effects on model and validation convergence.
Plot of the relation available in batch_sizes_vs_counts.png, all the other familiar logs, plots, and models available in their respective directories.
