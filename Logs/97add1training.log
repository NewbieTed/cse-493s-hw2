LR = 0.01, batch_size = 64, block_size = 16


number of parameters: 0.40M
Steps = 0, loss = 2.6145362854003906
Steps = 100, loss = 1.13477623462677
Steps = 200, loss = 1.1267293691635132
Steps = 300, loss = 1.0936932563781738
Steps = 400, loss = 1.1144567728042603
Steps = 500, loss = 1.1183632612228394
Steps = 600, loss = 1.093962550163269
Steps = 700, loss = 1.1046870946884155
Steps = 800, loss = 1.0995773077011108
Steps = 900, loss = 1.0719959735870361
Steps = 1000, loss = 1.0907883644104004
Steps = 1100, loss = 1.1065776348114014
Steps = 1200, loss = 1.100477933883667
Steps = 1300, loss = 1.109287142753601
Steps = 1400, loss = 1.7812365293502808
Steps = 1500, loss = 1.558242917060852
Steps = 1600, loss = 1.268613338470459
Steps = 1700, loss = 1.4570525884628296
Steps = 1800, loss = 1.5018447637557983
Steps = 1900, loss = 1.4401918649673462
Steps = 2000, loss = 1.409274935722351
Steps = 2100, loss = 1.4385312795639038
Steps = 2200, loss = 1.3544265031814575
Steps = 2300, loss = 1.3869755268096924
Steps = 2400, loss = 1.3589938879013062
Steps = 2500, loss = 1.4211698770523071
Steps = 2600, loss = 1.366375207901001
Steps = 2700, loss = 1.5564826726913452
Steps = 2800, loss = 1.29978346824646
Steps = 2900, loss = 1.1959104537963867
Steps = 3000, loss = 1.1329355239868164
Steps = 3100, loss = 1.1326425075531006
Steps = 3200, loss = 1.1007956266403198
Steps = 3300, loss = 1.1257203817367554
Steps = 3400, loss = 1.1329854726791382
Steps = 3500, loss = 1.119477391242981
Steps = 3600, loss = 1.10506272315979
Steps = 3700, loss = 1.114804983139038
Steps = 3800, loss = 1.128743052482605
Steps = 3900, loss = 1.121525764465332
Steps = 4000, loss = 1.12442946434021
Steps = 4100, loss = 1.1113961935043335
Steps = 4200, loss = 1.123415470123291
Steps = 4300, loss = 1.1186429262161255
Steps = 4400, loss = 1.1246541738510132
Steps = 4500, loss = 1.1084164381027222
Steps = 4600, loss = 1.1201367378234863
Steps = 4700, loss = 1.1051597595214844
Steps = 4800, loss = 1.0931910276412964
Steps = 4900, loss = 1.0858572721481323
Validation accuracy: 0.209375 (got 67 right out of 320)
